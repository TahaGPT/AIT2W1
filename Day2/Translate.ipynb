{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0328d6e7",
   "metadata": {},
   "source": [
    "Installing Packages and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da64ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (2.0.10)\n",
      "Requirement already satisfied: numpy in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (from pycocotools) (2.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  241M    0  103k    0     0  87895      0  0:47:57  0:00:01  0:47:56 87940\n",
      "  1  241M    1 2500k    0     0  1153k      0  0:03:34  0:00:02  0:03:32 1153k\n",
      "  2  241M    2 6692k    0     0  1993k      0  0:02:03  0:00:03  0:02:00 1994k\n",
      "  5  241M    5 12.1M    0     0  2993k      0  0:01:22  0:00:04  0:01:18 2993k\n",
      "  7  241M    7 18.9M    0     0  3743k      0  0:01:05  0:00:05  0:01:00 3945k\n",
      " 10  241M   10 26.2M    0     0  4361k      0  0:00:56  0:00:06  0:00:50 5400k\n",
      " 14  241M   14 33.9M    0     0  4848k      0  0:00:50  0:00:07  0:00:43 6449k\n",
      " 17  241M   17 41.9M    0     0  5257k      0  0:00:46  0:00:08  0:00:38 7534k\n",
      " 20  241M   20 50.3M    0     0  5617k      0  0:00:43  0:00:09  0:00:34 7806k\n",
      " 24  241M   24 58.3M    0     0  5877k      0  0:00:42  0:00:10  0:00:32 8084k\n",
      " 25  241M   25 62.5M    0     0  5705k      0  0:00:43  0:00:11  0:00:32 7346k\n",
      " 28  241M   28 69.4M    0     0  5847k      0  0:00:42  0:00:12  0:00:30 7281k\n",
      " 31  241M   31 76.2M    0     0  5925k      0  0:00:41  0:00:13  0:00:28 7016k\n",
      " 34  241M   34 83.1M    0     0  6012k      0  0:00:41  0:00:14  0:00:27 6735k\n",
      " 37  241M   37 90.3M    0     0  6097k      0  0:00:40  0:00:15  0:00:25 6543k\n",
      " 40  241M   40 97.0M    0     0  6148k      0  0:00:40  0:00:16  0:00:24 7151k\n",
      " 43  241M   43  103M    0     0  6199k      0  0:00:39  0:00:17  0:00:22 7054k\n",
      " 46  241M   46  111M    0     0  6258k      0  0:00:39  0:00:18  0:00:21 7134k\n",
      " 49  241M   49  118M    0     0  6321k      0  0:00:39  0:00:19  0:00:20 7198k\n",
      " 52  241M   52  125M    0     0  6381k      0  0:00:38  0:00:20  0:00:18 7242k\n",
      " 55  241M   55  133M    0     0  6435k      0  0:00:38  0:00:21  0:00:17 7365k\n",
      " 57  241M   57  137M    0     0  6355k      0  0:00:38  0:00:22  0:00:16 6890k\n",
      " 58  241M   58  140M    0     0  6189k      0  0:00:39  0:00:23  0:00:16 5940k\n",
      " 60  241M   60  144M    0     0  6138k      0  0:00:40  0:00:24  0:00:16 5434k\n",
      " 62  241M   62  151M    0     0  6144k      0  0:00:40  0:00:25  0:00:15 5189k\n",
      " 65  241M   65  157M    0     0  6167k      0  0:00:40  0:00:26  0:00:14 5034k\n",
      " 68  241M   68  164M    0     0  6202k      0  0:00:39  0:00:27  0:00:12 5525k\n",
      " 71  241M   71  171M    0     0  6235k      0  0:00:39  0:00:28  0:00:11 6449k\n",
      " 74  241M   74  178M    0     0  6279k      0  0:00:39  0:00:29  0:00:10 6959k\n",
      " 77  241M   77  185M    0     0  6312k      0  0:00:39  0:00:30  0:00:09 7160k\n",
      " 80  241M   80  193M    0     0  6342k      0  0:00:38  0:00:31  0:00:07 7257k\n",
      " 83  241M   83  200M    0     0  6379k      0  0:00:38  0:00:32  0:00:06 7341k\n",
      " 86  241M   86  207M    0     0  6409k      0  0:00:38  0:00:33  0:00:05 7391k\n",
      " 86  241M   86  209M    0     0  6276k      0  0:00:39  0:00:34  0:00:05 6257k\n",
      " 88  241M   88  213M    0     0  6168k      0  0:00:40  0:00:35  0:00:05 5336k\n",
      " 90  241M   90  217M    0     0  6152k      0  0:00:40  0:00:36  0:00:04 4964k\n",
      " 91  241M   91  221M    0     0  6103k      0  0:00:40  0:00:37  0:00:03 4326k\n",
      " 93  241M   93  225M    0     0  6058k      0  0:00:40  0:00:38  0:00:02 3733k\n",
      " 95  241M   95  230M    0     0  6023k      0  0:00:41  0:00:39  0:00:02 4295k\n",
      " 97  241M   97  234M    0     0  5982k      0  0:00:41  0:00:40  0:00:01 4600k\n",
      " 98  241M   98  238M    0     0  5935k      0  0:00:41  0:00:41 --:--:-- 4366k\n",
      " 99  241M   99  240M    0     0  5765k      0  0:00:42  0:00:42 --:--:-- 3465k\n",
      " 99  241M   99  240M    0     0  5684k      0  0:00:43  0:00:43 --:--:-- 2947k\n",
      "100  241M  100  241M    0     0  5690k      0  0:00:43  0:00:43 --:--:-- 2612k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  annotations_trainval2017.zip\n",
      "  inflating: annotations/instances_train2017.json  \n",
      "  inflating: annotations/instances_val2017.json  \n",
      "  inflating: annotations/captions_train2017.json  \n",
      "  inflating: annotations/captions_val2017.json  \n",
      "  inflating: annotations/person_keypoints_train2017.json  \n",
      "  inflating: annotations/person_keypoints_val2017.json  \n"
     ]
    }
   ],
   "source": [
    "%pip install pycocotools\n",
    "!curl -O http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a31a2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.3/42.3 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "     -------------------------------------- 187.3/187.3 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (from deep-translator) (2.32.4)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lab\\desktop\\kdd\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.6.15)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, deep-translator\n",
      "Successfully installed beautifulsoup4-4.13.4 deep-translator-1.11.4 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install deep-translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd7907",
   "metadata": {},
   "source": [
    "Using Googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b5004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading MS-COCO captions using pycocotools...\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "üåç Translating 20 captions into: ['Urdu', 'Arabic', 'Spanish', 'Chinese']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [01:11<00:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to coco_translated.jsonl...\n",
      "‚úÖ Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ---------- Config ----------\n",
    "ANNOTATION_FILE = 'annotations/captions_val2017.json'\n",
    "OUTPUT_FILE = 'coco_translated.jsonl'\n",
    "SAMPLE_SIZE = 20\n",
    "\n",
    "LANGUAGES = {\n",
    "    'ur': 'Urdu',\n",
    "    'ar': 'Arabic',\n",
    "    'es': 'Spanish',\n",
    "    'zh-cn': 'Chinese'\n",
    "}\n",
    "\n",
    "# ---------- Step 1: Load COCO ----------\n",
    "print(\"üìÇ Loading MS-COCO captions using pycocotools...\")\n",
    "coco = COCO(ANNOTATION_FILE)\n",
    "\n",
    "caption_ids = list(coco.anns.keys())[:SAMPLE_SIZE]  # first N captions\n",
    "translator = Translator()\n",
    "translated_data = []\n",
    "\n",
    "# ---------- Step 2: Translate Captions ----------\n",
    "print(f\"üåç Translating {SAMPLE_SIZE} captions into: {list(LANGUAGES.values())}\")\n",
    "\n",
    "for ann_id in tqdm(caption_ids):\n",
    "    ann = coco.anns[ann_id]\n",
    "    img_id = ann['image_id']\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_filename = img_info['file_name']\n",
    "    caption = ann['caption']\n",
    "\n",
    "    entry = {\n",
    "        \"image\": img_filename,\n",
    "        \"en_caption\": caption\n",
    "    }\n",
    "\n",
    "    for lang_code in LANGUAGES:\n",
    "        try:\n",
    "            translated = translator.translate(caption, dest=lang_code).text\n",
    "            entry[f\"{lang_code}_caption\"] = translated\n",
    "        except Exception as e:\n",
    "            entry[f\"{lang_code}_caption\"] = f\"[Translation failed: {e}]\"\n",
    "\n",
    "    translated_data.append(entry)\n",
    "\n",
    "# ---------- Step 3: Save as JSONL ----------\n",
    "print(f\"üíæ Saving to {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in translated_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c31c4",
   "metadata": {},
   "source": [
    "Using Deep translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0432c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading MS-COCO captions using pycocotools...\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "üåç Translating 20 captions into: ['Urdu', 'Arabic', 'Spanish', 'Chinese']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:43<00:00,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to coco_translated_Deep.jsonl...\n",
      "‚úÖ Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ---------- Config ----------\n",
    "ANNOTATION_FILE = 'annotations/captions_val2017.json'\n",
    "OUTPUT_FILE = 'coco_translated_Deep.jsonl'\n",
    "SAMPLE_SIZE = 20\n",
    "\n",
    "LANGUAGES = {\n",
    "    'ur': 'Urdu',\n",
    "    'ar': 'Arabic',\n",
    "    'es': 'Spanish',\n",
    "    'zh-cn': 'Chinese'\n",
    "}\n",
    "\n",
    "# ---------- Step 1: Load COCO ----------\n",
    "print(\"üìÇ Loading MS-COCO captions using pycocotools...\")\n",
    "coco = COCO(ANNOTATION_FILE)\n",
    "\n",
    "caption_ids = list(coco.anns.keys())[:SAMPLE_SIZE]\n",
    "translated_data = []\n",
    "\n",
    "# ---------- Step 2: Translate Captions ----------\n",
    "print(f\"üåç Translating {SAMPLE_SIZE} captions into: {list(LANGUAGES.values())}\")\n",
    "\n",
    "for ann_id in tqdm(caption_ids):\n",
    "    ann = coco.anns[ann_id]\n",
    "    img_id = ann['image_id']\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_filename = img_info['file_name']\n",
    "    caption = ann['caption']\n",
    "\n",
    "    entry = {\n",
    "        \"image\": img_filename,\n",
    "        \"en_caption\": caption\n",
    "    }\n",
    "\n",
    "    for lang_code in LANGUAGES:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source='en', target=lang_code).translate(caption)\n",
    "            entry[f\"{lang_code}_caption\"] = translated\n",
    "        except Exception as e:\n",
    "            entry[f\"{lang_code}_caption\"] = f\"[Translation failed: {e}]\"\n",
    "\n",
    "    translated_data.append(entry)\n",
    "\n",
    "# ---------- Step 3: Save as JSONL ----------\n",
    "print(f\"üíæ Saving to {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in translated_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
